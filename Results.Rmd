---
title: "Results"
author: "Dzemiantsova"
date: "September 11, 2015"
output: html_document
---
Libraries:
```{r, echo=FALSE}
library(gam)
library(caret)
library(kernlab)
library(doParallel)

checkColumn <- function(data.1, data.2){
   k <- vector()
   index <- vector()
   m <- 1
   for(i in 1:dim(data.1)[2]){
      a <- 0
      for(j in 1:dim(data.2)[2]){
         if(names(data.1)[i] == names(data.2)[j]) a <- a + 1
      }
      if(a == 0){
         k[m] <- names(data.1)[i]
         index[m] <- which(names(data.1) == names(data.1)[i])
         m <- m + 1
      }
   } 
   index  
}
```

I save the training and the testing data in the training and the testing data frame, respectively, and check their structure and dimension.
```{r}
training <-  read.csv("pml-training.csv", na.strings=c("NA",""))
testing <-  read.csv("pml-testing.csv", na.strings=c("NA",""))
#str(training)
#str(testing)
dim(training)
dim(testing)
```

First, I explore the training data. There are missing values NA which I replace with the mean of the non-missings in case of continuous variables, and add a new level "NA" in case of factors.
```{r}
training <- na.gam.replace(training)
```

With the research article given by Coursera, I check the description of each predictor in the data. I find that the first five predictors such as index, timestamp and username, are not important. Therefore, I consider only the rest predictors which I sort alphabetically:
```{r}
training <- training[,6:160]
training <- training[,order(names(training))]
testing <- testing[,6:160]
testing <- testing[,order(names(testing))]
```

Interestingly, the number of predictors in the training is equal to those in the testing, though there is no 'classe' column in the testing. This means that there is a predictor in the testing that does not exist in the training. To find this predictor, I use a function 'checkColumn' written by myself, which returns indices of columns that are missing in one of two data sets.  

```{r}
training.index <- checkColumn(training, testing)
testing.index <- checkColumn(testing, training) 
```

As a result, training.index returns an index of the 'classe' column in the training, whereas testing.index resturns an index of the 'problem_id' column in the testing.
```{r}
training.index
testing.index
```

I save the 'classe' column in a new vector 'trainClass', and remove the 'classe' and the 'problem_id' column from the training and the testing data, respectively.
```{r}
trainClass <- training[, training.index]
training <- training[, -training.index]
testing <- testing[, -testing.index]
```

Since both data sets have the same predictors, I can pre-process the data in the same way. I remove a near zero-variance predictors:
```{r}
zeroTrain <- nearZeroVar(training)
training <- training[, -zeroTrain]
testing <- testing[, -zeroTrain]
```

I also remove those predictors that result in absolute pairwise correlation greater than a threshold of 0.9.
```{r}
Corr <- cor(training)
highCorr <- findCorrelation(Corr, 0.9)
training <- training[, -highCorr]
testing <- testing[, -highCorr]
```

Last, the predictor variables are centered and scaled.
```{r}
xTrans <- preProcess(training)
training <- predict(xTrans, training)
testing <- predict(xTrans, testing)
```

In order to make classification predictions for the testing data, I need to build a model trained on the training data. First, I split the training data into two sets: in.sample and out.of.sample.
```{r}
inTrain <- createDataPartition(y = trainClass, p = 0.6, list = FALSE)
in.sample <- training[inTrain,]
out.of.sample <- training[-inTrain,]
```

Next, I need to choose a supervised learning model. In case of a classification problem with a number of classes more than two, I use the random forest (rf). To avoid overfitting and make the most efficient use of the data, I train the model with the 5-fold cross-validation. 
```{r, echo=FALSE}
registerDoParallel(cores=2)
```

```{r}
cvControl <- trainControl(method = "repeatedcv", repeats=5)
set.seed(12345)
rfFit <- train(in.sample, trainClass[inTrain], method = "rf", trControl = cvControl)
print(rfFit$finalModel)
```

The model shows good results, where the in-sample (training) error is less than 1 %. Since it is not equal to zero, there is no overfitting. To estimate the efficacy of the model, I check a confusion matrix for the out-of-sample data and calculate an out-of-sample error. 
```{r}
predictions <- predict(rfFit$finalModel, newdata = out.of.sample)
confusionMatrix(predictions, trainClass[-inTrain])
outofsample.error <- mean(predictions != trainClass[-inTrain])
paste("out-of-sample error: ",outofsample.error)
```

As it is expected, the out-of-sample error is larger compared to the in-sample error. Since the out-of-sample error is still less than 1 %, the model can predict 'classe' for the testing data with the accuracy of almost 100 %.

