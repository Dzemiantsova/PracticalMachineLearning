---
title: "Coursera Practical Machine Learning Project"
author: "Liudmila Dzemiantsova"
date: "September 11, 2015"
output: html_document
---
**Summary** 

This project compare several machine learning algorithms to predict exercise quality based on the activity data monitored by accelerometers (http://groupware.les.inf.puc-rio.br/har). Results obtained by Support Vector Machines (SVM), K-Nearest-Neighbor (KNN) and Random Forest (RF) are compared. Based on out-of-sample errors and the errors with cross-validation, RF provides the best prediction accuracy, with an out-of-sample error < 0.05%, and a cross-validation error < 0.1%. Results based on 20 testing samples show that the prediction model built based on RF correctly predicts the activity quality of all these samples.

**Libraries**
```{r, echo=FALSE}
library(gam)
library(caret)
library(kernlab)
library(randomForest)
library(doParallel)

checkColumn <- function(data.1, data.2){
   k <- vector()
   index <- vector()
   m <- 1
   for(i in 1:dim(data.1)[2]){
      a <- 0
      for(j in 1:dim(data.2)[2]){
         if(names(data.1)[i] == names(data.2)[j]) a <- a + 1
      }
      if(a == 0){
         k[m] <- names(data.1)[i]
         index[m] <- which(names(data.1) == names(data.1)[i])
         m <- m + 1
      }
   } 
   index  
}
```
**Read and clean the data**

```{r}
training <-  read.csv("pml-training.csv", na.strings=c("NA",""))
testing <-  read.csv("pml-testing.csv", na.strings=c("NA",""))
#str(training)
#str(testing)
dim(training)
dim(testing)
```

First, the training data are explored. There are missing values NA which I replace with the mean of the non-missings in case of continuous variables, and add a new level "NA" in case of factors.
```{r}
training <- na.gam.replace(training)
```

I eliminate predictors such as index, timestamp and username, which are not important for the data, and sort the rest predictors alphabetically:
```{r}
training <- training[,6:160]
training <- training[,order(names(training))]
testing <- testing[,6:160]
testing <- testing[,order(names(testing))]
```

Interestingly, the number of predictors in the training is equal to those in the testing, though there is no 'classe' column in the testing. This means that there is a predictor in the testing that does not exist in the training. To find this predictor, I use my own written function 'checkColumn' that returns indices of columns that are missing in one of two data sets.  

```{r}
training.index <- checkColumn(training, testing)
testing.index <- checkColumn(testing, training) 
```

As a result, training.index returns an index of the 'classe' column in the training, whereas testing.index resturns an index of the 'problem_id' column in the testing. I save the 'classe' column in a new vector 'trainClass', and remove the 'classe' and the 'problem_id' column from the training and the testing data, respectively.

```{r}
trainClass <- training[, training.index]
training <- training[, -training.index]
testing <- testing[, -testing.index]
```

**Pre-processing the data**

Since both data sets have the same predictors, I can pre-process the data in the same way. I remove a near zero-variance predictors:
```{r}
zeroTrain <- nearZeroVar(training)
training <- training[, -zeroTrain]
testing <- testing[, -zeroTrain]
```

I also remove those predictors that result in absolute pairwise correlation greater than a threshold of 0.9.
```{r}
Corr <- cor(training)
highCorr <- findCorrelation(Corr, 0.9)
training <- training[, -highCorr]
testing <- testing[, -highCorr]
```

Last, the predictor variables are centered and scaled.
```{r}
xTrans <- preProcess(training)
training <- predict(xTrans, training)
testing <- predict(xTrans, testing)
```
**Building models**

The 40% of the data in the original training data set will be used to estimate out-of-sample. These samples will not be used for building prediction models, while the remaining 60% data (in-sample) in the original training data set will be used to build the machine learning models. 
```{r}
inTrain <- createDataPartition(y = trainClass, p = 0.6, list = FALSE)
in.sample <- training[inTrain,]
out.of.sample <- training[-inTrain,]
```

In case of a classification problem with a number of classes more than two, I use different supervised lerning methods: SVM, KNN and RF. To avoid overfitting and make the most efficient use of the data, I train the model with the 5-fold cross-validation. 
```{r, echo=FALSE}
registerDoParallel(cores=2)
```

SVM:
```{r}
cvControl <- trainControl(method = "repeatedcv", repeats=5)
set.seed(12345)
if (file.exists("svmFit.rda")) {
    load("svmFit.rda")
  } else {
    svmFit <- train(in.sample, trainClass[inTrain], method = "svmRadial", trControl = cvControl)
    save(svmFit, file="svmFit.rda")
    }
print(svmFit$finalModel)
```

KNN:
```{r}
knnGrid <- expand.grid(.k=c(2:5))
set.seed(12345)
if (file.exists("knnFit.rda")) {
    load("knnFit.rda")
  } else {
    knnFit <- train(in.sample, trainClass[inTrain], method = "knn", trControl = cvControl, tuneGrid = knnGrid)
    save(knnFit, file="knnFit.rda")
    }
print(knnFit$finalModel)
```

RF:
```{r}
set.seed(12345)
if (file.exists("rfFit.rda")) {
    load("rfFit.rda")
  } else {
    rfFit <- train(in.sample, trainClass[inTrain], method = "rf", trControl = cvControl)
    save(rfFit, file="rfFit.rda")
    }
print(rfFit)
```
The results show that the RF model has a high CV accuracy corresponding to a low CV error.  

**Compare the modes**

I compare the models by errors of CV and out-of-sample errors.
```{r}
set.seed(1)
predictions.svm <- predict(svmFit, newdata = out.of.sample)
set.seed(1)
predictions.knn <- predict(knnFit$finalModel, newdata = out.of.sample)
set.seed(1)
predictions.rf <- predict(rfFit, newdata = out.of.sample)
```

```{r}
svm.model <- data.frame(model = "svm", "error"=mean(predictions.svm != trainClass[-inTrain]))
knn.model <- data.frame(model = "knn", "error"=mean(predictions.knn != trainClass[-inTrain]))
rf.model <- data.frame(model = "rf", "error"=mean(predictions.rf != trainClass[-inTrain]))
all.model <- rbind(svm.model, knn.model, rf.model)
all.model
```

The SVM and RF models show best results, where the in-sample (training) errors are less than 1 %. Since out-of-sample errors of all models are comparable with corresponding in-sample errors, there are no overfitting. Results show that RF model provides a high accuracy corresponding to a much lower out-of-sample error (<0.05%), and therefore is used for predicting the testing data set. Classifications for all the 20 test samples are correctly predicted. 
```{r}
set.seed(1)
predict(rfFit, newdata = testing)
```




